# ML-CV-Ideas

Like a lot of other machine learning & computer vision researchers, I occasionally stumble across a "wouldn't it be cool it try this?" idea. I wanted a centralized, non-device specific place to store some of my ramblings, with no expectation of completeness or that it's even remotely not ridiculous, that I could easily share with others to get some feedback (or be told it's already been done before :) ). Below is the list of these ramblings - apologies for any lack of proper grammar, coherency or sanity.

* Learned Augmentation for Domain Adaptation
  * Problem: Given a source dataset with lots of samples and labels, can we learn well on a target with either few samples, or lots of samples but few labels?
  * Many approaches focus on changing the feature extractor (starting from a source) to extract better on the target (like [MinimaxEntropy](https://arxiv.org/abs/1904.06487) or [Fully Test Time Adaptation](https://arxiv.org/abs/2006.10726)). However, changing a feature extractor can be problematic, especially if we have few samples / labels. One solution could be to augment the images in the target domain to more closely resemble the images in the source domain. Given a classifier trained on source data (CS), we want some operation G that takes in an image and outputs some similarly shaped image (like an autoencoder). If G(target data) ~ source data, then CS(G(target data)) will classify on target data. (Classifier is used here for simplicity, although the larger point is the underlying feature extractor rather than the specific task).
  * An initial attemp at this could be: given two labelled datasets (source and target) and a classifier trained on source, train CS(G(target data)), where the weights for CS are either frozen or update very slowly. Obviously some choices need to be made on choosing G, and there's some more room for more significant experimentation.
* Learned Augmentation for Contrastive Learning
  * Something similar to the above idea but learn one (or both) of the augmentations for contrastive learning.
  * One example: Do [FixMatch](https://arxiv.org/abs/2001.07685) but learn the strong augmentation (FixMatch Diagram is below). This can be made more complicated by having various complexities to G (G2 is a stronger augmentation than G1, G3 than G2, ....)  and then stack each successive augmentation. (ex: For the 1st epoch, have rotation be the weak augmentation, G1 be the strong augmentation. 2nd epoch - G1 = weak augmentation, G2 = strong augmentation ...).
 ![FixMatch Diagram](https://raw.githubusercontent.com/google-research/fixmatch/master/media/FixMatch%20diagram.png)
* Copernician Learner
* Explore underspecification for unsupervised learning and different domain adaptation techniques
* Explicitly Learned Salience
  * Saliency maps are used for explainability to highlight what's important in the image (what the model is focusing on). Saliency maps share a lot of similarities to segmentation masks. An interesting approach therefore would be to explicitly learn salience, like a segmentation task. 
  * A fairly simple, if almost boring experiment would be to take a segmentation dataset, and train a U-Net (or your choice of segmentation network) on this as a binary task (object vs background, which depending on the dataset may represent salient vs not). Then use the embedded features to train a classifier (ex: a linear layer on top of the U-Net encoder, where the encoder weights are shared between the classifier network and segmentation network). The classifier and segmenter could either be trained in series or parallel (if done in parallel, some thought will need to be given on handling this elegantly). The mildly interesting part of the experiment would be to see how training order affects classification and segmentation results and generalizability (3 options - train classifier then segmenter. train segmenter then classifier, or train both in parallel). At inference, given an image, we can run it through the classifier to get its classification, and also through the whole segmentation network to get it's salience. (This can be extended to add object detection as well for fun).
  * The above by itself isn't particularly interesting (basically given a segmentation task can we do classification, which doesn't really add value). What would be a lot more fun would be - given a classifier, can we train a segmenter that gives us salience? Going beyond this, use the concatenations in the U-Net to learn saliency.
* Inverted U-Net
  * Playing off the saliency map idea, given a segmentation map, can we reconstruct the original image? (Basically train a unet as normal, but flip the inputs and outputs). Not entirely sure what the benefit of this would be other than just curious what would happen (we wouldn't expect it to be able to reconstruct a full image from just a mask, as there are a lot of extraneous details that you just can't get - the problem is severely underspecified.)
